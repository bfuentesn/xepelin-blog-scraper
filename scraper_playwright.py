#!/usr/bin/env python3
"""
Scraper con Playwright para obtener TODOS los posts del blog de Xepelin.
Requiere dependencias del sistema instaladas: sudo playwright install-deps
"""
import time
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from playwright.sync_api import sync_playwright, Page, Browser, TimeoutError as PlaywrightTimeout


class XepelinPlaywrightScraper:
    """
    Scraper que usa Playwright para manejar carga din√°mica del blog.
    Capaz de obtener TODOS los posts, incluyendo los cargados con el bot√≥n "Cargar m√°s".
    """
    
    BASE_URL = "https://xepelin.com/blog"
    
    CATEGORIES = {
        "Pymes": "pymes",
        "Corporativos": "corporativos",
        "Educaci√≥n Financiera": "educacion-financiera",
        "Emprendedores": "emprendedores",
        "Noticias": "noticias",
        "Casos de √©xito": "empresarios-exitosos"
    }
    
    def __init__(self, headless: bool = True, timeout: int = 60000):
        """
        Inicializa el scraper con Playwright.
        
        Args:
            headless: Si True, ejecuta el navegador sin GUI
            timeout: Timeout en milisegundos para operaciones de p√°gina
        """
        self.headless = headless
        self.timeout = timeout
        self.browser: Optional[Browser] = None
        self.playwright = None
    
    def __enter__(self):
        """Context manager para manejar el navegador."""
        import os
        
        # Verificar que Playwright puede encontrar los browsers
        browsers_path = os.environ.get('PLAYWRIGHT_BROWSERS_PATH', '/opt/render/.cache/ms-playwright')
        if os.path.exists(browsers_path):
            print(f"‚úÖ Playwright browsers path exists: {browsers_path}")
            # Listar contenido para debug
            try:
                import subprocess
                result = subprocess.run(['ls', '-la', browsers_path], capture_output=True, text=True)
                print(f"Contents:\n{result.stdout}")
            except Exception as e:
                print(f"Could not list directory: {e}")
        else:
            print(f"‚ö†Ô∏è  WARNING: Playwright browsers path does not exist: {browsers_path}")
        
        self.playwright = sync_playwright().start()
        print("üé≠ Launching Chromium browser...")
        self.browser = self.playwright.chromium.launch(headless=self.headless)
        print("‚úÖ Chromium browser launched successfully")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Cierra el navegador al salir."""
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
    
    def _load_all_posts(self, page: Page) -> None:
        """
        Hace scroll y carga todos los posts clickeando "Cargar m√°s" hasta que no haya m√°s.
        
        Args:
            page: P√°gina de Playwright
        """
        max_clicks = 100  # L√≠mite de seguridad para evitar loops infinitos
        clicks = 0
        no_change_count = 0  # Contador de iteraciones sin cambios
        
        print("üîÑ Cargando todos los posts...")
        
        while clicks < max_clicks:
            try:
                # Contar posts antes del scroll
                posts_before = page.locator('a[href*="/blog/"][href*="-"]').count()
                print(f"   üìä Posts antes del scroll: {posts_before}")
                
                # Hacer scroll hasta el final de la p√°gina
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                print(f"   ‚¨áÔ∏è  Scroll al final de la p√°gina...")
                
                # Esperar generosamente a que se cargue contenido
                time.sleep(5)
                
                # Contar posts despu√©s del scroll
                posts_after_scroll = page.locator('a[href*="/blog/"][href*="-"]').count()
                print(f"   üìä Posts despu√©s del scroll: {posts_after_scroll}")
                
                # Si el scroll solo ya carg√≥ m√°s posts, continuar
                if posts_after_scroll > posts_before:
                    print(f"   ‚úÖ Se cargaron {posts_after_scroll - posts_before} posts nuevos con el scroll")
                    no_change_count = 0
                    clicks += 1
                    continue
                
                # Buscar el bot√≥n "Cargar m√°s"
                load_more_button = page.locator('button:has-text("Cargar m√°s")').first
                button_count = load_more_button.count()
                
                print(f"   üîç Botones 'Cargar m√°s' encontrados: {button_count}")
                
                if button_count > 0:
                    try:
                        # Verificar si es visible (sin timeout largo)
                        is_visible = load_more_button.is_visible(timeout=2000)
                        print(f"   üëÅÔ∏è  Bot√≥n visible: {is_visible}")
                        
                        if is_visible:
                            print(f"   üñ±Ô∏è  Haciendo clic en 'Cargar m√°s' (intento #{clicks + 1})...")
                            
                            # Scroll al bot√≥n para asegurar que est√© en viewport
                            load_more_button.scroll_into_view_if_needed()
                            time.sleep(1)
                            
                            # Hacer clic
                            load_more_button.click()
                            print(f"   ‚úÖ Clic realizado")
                            
                            # Esperar a que carguen nuevos posts
                            time.sleep(5)
                            
                            # Verificar si se cargaron posts nuevos
                            posts_after_click = page.locator('a[href*="/blog/"][href*="-"]').count()
                            new_posts = posts_after_click - posts_after_scroll
                            print(f"   üìä Posts despu√©s del clic: {posts_after_click} (+{new_posts})")
                            
                            if new_posts > 0:
                                no_change_count = 0
                                clicks += 1
                            else:
                                no_change_count += 1
                                print(f"   ‚ö†Ô∏è  No se cargaron posts nuevos (contador: {no_change_count})")
                        else:
                            print(f"   ‚ÑπÔ∏è  Bot√≥n encontrado pero no visible")
                            no_change_count += 1
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Error al interactuar con bot√≥n: {str(e)[:150]}")
                        no_change_count += 1
                else:
                    print(f"   ‚ÑπÔ∏è  No se encontr√≥ bot√≥n 'Cargar m√°s'")
                    no_change_count += 1
                
                # Si no hubo cambios en 3 iteraciones, terminar
                if no_change_count >= 3:
                    print(f"   ‚úÖ Terminado: No hay m√°s posts para cargar (sin cambios x{no_change_count})")
                    break
                    
            except PlaywrightTimeout:
                # No se encontr√≥ el bot√≥n o no es visible
                print("‚úÖ No hay m√°s posts para cargar (timeout)")
                break
            except Exception as e:
                print(f"‚ö†Ô∏è Error al cargar m√°s posts: {e}")
                break
        
        if clicks >= max_clicks:
            print(f"‚ö†Ô∏è Se alcanz√≥ el l√≠mite de {max_clicks} clics (puedes aumentarlo en el c√≥digo si necesitas m√°s)")
        
        # Hacer un √∫ltimo scroll para asegurar que todo est√© cargado
        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        time.sleep(2)
    
    def _extract_post_details(self, page: Page, url: str) -> Dict[str, str]:
        """
        Navega a un post individual para extraer sus detalles completos.
        
        Args:
            page: P√°gina de Playwright
            url: URL del post
            
        Returns:
            Diccionario con los datos del post
        """
        try:
            # Navegar al post
            page.goto(url, wait_until="domcontentloaded", timeout=30000)
            page.wait_for_timeout(1000)  # Reducido de 2000 a 1000ms para mayor velocidad
            
            html = page.content()
            soup = BeautifulSoup(html, 'lxml')
            
            # Extraer t√≠tulo
            titulo = "Sin t√≠tulo"
            for tag in ['h1', 'h2']:
                title_tag = soup.find(tag)
                if title_tag:
                    titulo = title_tag.get_text(strip=True)
                    break
            
            # Extraer tiempo de lectura (debajo del t√≠tulo)
            # Usar el selector CSS espec√≠fico: div con clase Text_body__snVk8
            tiempo_lectura = "N/A"
            try:
                tiempo_div = soup.find('div', class_='Text_body__snVk8')
                if tiempo_div:
                    tiempo_lectura = tiempo_div.get_text(strip=True)
                    # Limpiar espacios: "7min de lectura" -> "7 min de lectura"
                    tiempo_lectura = tiempo_lectura.replace('min de lectura', ' min de lectura').strip()
            except Exception:
                pass
            
            # Extraer autor (debajo de la imagen principal)
            # Buscar el contenedor con clase 'flex gap-2' que contiene los divs del autor
            autor = "N/A"
            try:
                author_container = soup.find('div', class_='flex gap-2')
                if author_container:
                    # Encontrar todos los divs con el texto del autor
                    all_divs = author_container.find_all('div', class_='text-sm dark:text-text-disabled')
                    autor_parts = [div.get_text(strip=True) for div in all_divs if div.get_text(strip=True)]
                    if autor_parts:
                        autor = ' '.join(autor_parts)  # "Lilia Valenzuela | SaaS Specialist"
            except Exception:
                pass
            
            # Extraer fecha de publicaci√≥n desde metadata
            # NOTA: El sitio web no expone fechas de publicaci√≥n en metadata est√°ndar
            # ni en JSON-LD. Las fechas est√°n embebidas en el contenido Next.js
            # pero no son accesibles mediante scraping est√°ndar.
            fecha = "N/A"
            try:
                # Intentar obtener desde meta tag primero
                meta_date = soup.find('meta', property='article:published_time')
                if meta_date:
                    fecha = meta_date.get('content', 'N/A')
                else:
                    # Intentar desde JSON-LD
                    json_ld = soup.find('script', type='application/ld+json')
                    if json_ld and json_ld.string:
                        import json
                        data = json.loads(json_ld.string)
                        if isinstance(data, dict) and 'datePublished' in data:
                            fecha = data['datePublished']
            except Exception:
                pass
            
            return {
                "Titular": titulo,
                "Autor": autor,
                "Tiempo de lectura": tiempo_lectura,
                "Fecha": fecha,
                "URL": url
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error extrayendo detalles de {url}: {str(e)}")
            # Retornar datos b√°sicos en caso de error
            return {
                "Titular": url.split('/')[-1].replace('-', ' ').title(),
                "Autor": "N/A",
                "Tiempo de lectura": "N/A",
                "Fecha": "N/A",
                "URL": url
            }
    
    def _extract_posts_from_page(self, initial_page: Page) -> List[Dict[str, str]]:
        """
        Extrae todos los posts de la p√°gina actual usando BeautifulSoup despu√©s de la carga din√°mica.
        
        Args:
            initial_page: P√°gina de Playwright con los posts cargados
            
        Returns:
            Lista de diccionarios con la informaci√≥n de cada post
        """
        page = initial_page
        # Obtener el HTML completo despu√©s de la carga din√°mica
        html_content = page.content()
        
        # Parsear con BeautifulSoup
        soup = BeautifulSoup(html_content, 'lxml')
        
        posts = []
        seen_urls = set()
        
        # Buscar todos los enlaces a posts
        post_links = soup.find_all('a', href=lambda x: x and '/blog/' in x and '-' in x)
        print(f"üìä Encontrados {len(post_links)} enlaces potenciales")
        
        # DEBUG: Print all candidate URLs before filtering
        print(f"üîç DEBUG: Raw candidate hrefs: {[link.get('href', '') for link in post_links]}")
        
        # Primero recolectar todas las URLs
        urls_to_process = []
        for link in post_links:
            try:
                url = link.get('href', '')
                
                # Validar URL
                if not url or not url.startswith('http'):
                    if url.startswith('/'):
                        url = f"https://xepelin.com{url}"
                    else:
                        continue
                
                # Evitar p√°ginas de categor√≠as (exactas, sin posts despu√©s)
                # Agregar trailing slash para comparaci√≥n exacta
                url_clean = url.rstrip('/')
                category_pages = [
                    'https://xepelin.com/blog/pymes',
                    'https://xepelin.com/blog/corporativos',
                    'https://xepelin.com/blog/educacion-financiera',
                    'https://xepelin.com/blog/emprendedores',
                    'https://xepelin.com/blog/noticias',
                    'https://xepelin.com/blog/empresarios-exitosos',
                    'https://xepelin.com/blog'
                ]
                if url_clean in category_pages:
                    continue
                
                # Evitar duplicados
                if url in seen_urls:
                    continue
                
                seen_urls.add(url)
                urls_to_process.append(url)
                
            except Exception as e:
                continue
        
        print(f"üìã Procesando {len(urls_to_process)} posts individuales para extraer detalles completos...")
        print(f"üîç DEBUG: First 3 URLs: {urls_to_process[:3] if urls_to_process else 'None'}")
        
        # Ahora navegar a cada post para obtener detalles
        # IMPORTANTE: Reiniciar p√°gina cada 100 posts para liberar memoria
        for i, url in enumerate(urls_to_process, 1):
            if i % 10 == 0:
                print(f"   Procesados {i}/{len(urls_to_process)} posts...")
            
            # Reiniciar p√°gina cada 100 posts para prevenir "object has been collected"
            if i > 1 and i % 100 == 0 and i < len(urls_to_process):
                print(f"   üîÑ Reiniciando p√°gina para liberar memoria (post {i}/{len(urls_to_process)})...")
                try:
                    # Cerrar p√°gina actual
                    page.close()
                    # Crear nueva p√°gina limpia
                    page = self.browser.new_page()
                    page.set_default_timeout(self.timeout)
                    print(f"   ‚úÖ P√°gina reiniciada, continuando...")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Error reiniciando p√°gina: {e}")
            
            try:
                post_details = self._extract_post_details(page, url)
                if post_details:
                    posts.append(post_details)
            except Exception as e:
                print(f"   ‚ö†Ô∏è Error procesando post {i}: {str(e)}")
                continue
        
        print(f"‚úÖ {len(posts)} posts √∫nicos extra√≠dos con detalles completos")
        return posts
    
    def scrape_category(self, category_name: str) -> List[Dict[str, str]]:
        """
        Scrapea TODOS los posts de una categor√≠a espec√≠fica.
        
        Args:
            category_name: Nombre de la categor√≠a (ej: "Pymes")
            
        Returns:
            Lista de diccionarios con los posts de la categor√≠a
        """
        if category_name not in self.CATEGORIES:
            raise ValueError(f"Categor√≠a '{category_name}' no v√°lida. "
                           f"Categor√≠as disponibles: {list(self.CATEGORIES.keys())}")
        
        category_slug = self.CATEGORIES[category_name]
        url = f"{self.BASE_URL}/{category_slug}"
        
        print(f"\nüéØ Scrapeando categor√≠a: {category_name}")
        print(f"üìç URL: {url}")
        
        if not self.browser:
            raise RuntimeError("Browser no inicializado. Usa 'with XepelinPlaywrightScraper():'")
        
        # Crear una nueva p√°gina
        page = self.browser.new_page()
        page.set_default_timeout(self.timeout)
        
        try:
            # Navegar a la p√°gina de la categor√≠a
            print(f"üåê Navegando a {url}...")
            page.goto(url, wait_until="networkidle", timeout=60000)
            print("‚úÖ P√°gina cargada")
            
            # Hacer un scroll inicial para activar el lazy loading
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            
            # Esperar a que aparezcan los enlaces de posts (con hyphens en la URL)
            print("‚è≥ Esperando a que se carguen los posts...")
            try:
                # Esperar hasta 15 segundos a que aparezca al menos un enlace de post
                page.wait_for_selector('a[href*="/blog/"][href*="-"]', timeout=15000)
                print("‚úÖ Posts encontrados en la p√°gina")
            except PlaywrightTimeout:
                print("‚ö†Ô∏è  Timeout esperando posts - intentando continuar de todos modos")
            
            # Esperar m√°s tiempo para que el contenido se renderice completamente
            page.wait_for_timeout(5000)
            
            # Cargar todos los posts
            self._load_all_posts(page)
            
            # Extraer posts
            posts = self._extract_posts_from_page(page)
            
            # Asignar categor√≠a correcta a todos los posts
            for post in posts:
                post["Categor√≠a"] = category_name
            
            print(f"‚úÖ {len(posts)} posts extra√≠dos de {category_name}")
            return posts
            
        finally:
            page.close()
    
    def scrape_all_categories(self) -> Dict[str, List[Dict[str, str]]]:
        """
        Scrapea TODOS los posts de TODAS las categor√≠as.
        
        Returns:
            Diccionario con categor√≠as como keys y listas de posts como values
        """
        results = {}
        
        print("\n" + "="*70)
        print("üöÄ INICIANDO SCRAPING COMPLETO DE TODAS LAS CATEGOR√çAS")
        print("="*70)
        
        for category_name in self.CATEGORIES.keys():
            try:
                posts = self.scrape_category(category_name)
                results[category_name] = posts
            except Exception as e:
                print(f"‚ùå Error scrapeando {category_name}: {e}")
                results[category_name] = []
        
        # Resumen
        total_posts = sum(len(posts) for posts in results.values())
        print("\n" + "="*70)
        print(f"‚ú® SCRAPING COMPLETO - Total: {total_posts} posts")
        print("="*70)
        for cat, posts in results.items():
            print(f"  ‚Ä¢ {cat}: {len(posts)} posts")
        print("="*70 + "\n")
        
        return results


def test_scraper():
    """Funci√≥n de prueba para el scraper."""
    print("üß™ Probando Playwright Scraper...\n")
    
    try:
        with XepelinPlaywrightScraper(headless=True) as scraper:
            # Probar con una categor√≠a
            posts = scraper.scrape_category("Pymes")
            
            if posts:
                print(f"\n‚úÖ {len(posts)} posts encontrados en Pymes")
                print("\nPrimeros 3 posts:")
                for i, post in enumerate(posts[:3], 1):
                    print(f"\n{i}. {post['Titular']}")
                    print(f"   Autor: {post['Autor']}")
                    print(f"   Fecha: {post['Fecha']}")
                    print(f"   URL: {post['URL']}")
            else:
                print("‚ùå No se encontraron posts")
    
    except Exception as e:
        print(f"\n‚ùå ERROR: {e}")
        print("\nüí° Si ves 'Host system is missing dependencies', ejecuta:")
        print("   sudo playwright install-deps")


if __name__ == "__main__":
    test_scraper()
